{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71e5f7c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2dd2f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "api_key = os.environ[\"GOOGLE_API_KEY\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb83df53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etapa 1 - ETL\n",
    "\n",
    "# SUBIR OS PDFs\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "\n",
    "loader = PyPDFDirectoryLoader(\"documentos_curso/\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Gerar os chunks e fazer o embeding\n",
    "\n",
    "\n",
    "# Subi para o banco de dados chromaDb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c098a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,\n",
    "    chunk_overlap=200,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "for i, chunk in enumerate(chunks, start=1):\n",
    "    print(f\"Chunk {i} – origem: {chunk.metadata.get('source')}\")\n",
    "    print(chunk.page_content)\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a2b24041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GoogleGenerativeAIEmbeddings(client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x000001EFDA3078A0>, model='models/embedding-001', task_type=None, google_api_key=SecretStr('**********'), credentials=None, client_options=None, transport=None, request_options=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", api_key= api_key)\n",
    "embeddings_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ed04324c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Banco de dados criado com sucesso!\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings_model)\n",
    "\n",
    "print(\"Banco de dados criado com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "82759e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recuperador de Busca Hibrida Configurado\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "\n",
    "# 1. Recuperador Lexical (BM25)\n",
    "bm25_retriever = BM25Retriever.from_documents(chunks)\n",
    "bm25_retriever.k = 5\n",
    "\n",
    "# 2. Recuperador Vetorial (a partir do nosso ChromaDB)\n",
    "vector_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "\n",
    "# 3. EnsembleRetriever para combinar os resultados\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, vector_retriever],\n",
    "    weights=[0.4, 0.6]\n",
    ")\n",
    "\n",
    "print(\"Recuperador de Busca Hibrida Configurado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a6757d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"models/gemini-2.0-flash\", temperature=0, api_key=api_key)\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    output_key=\"answer\",\n",
    "    return_messages=True)\n",
    "\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=ensemble_retriever,\n",
    "    memory=memory,\n",
    "    verbose=False,\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "006f23a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking adaptativo é a técnica de dividir documentos longos em pedaços (chunks) de forma inteligente, preservando o contexto semântico. A estratégia de chunking deve ser adaptada ao tipo de dado, como textos contínuos, tabelas, código-fonte ou documentos estruturados, que requerem abordagens diferentes. As abordagens incluem: Fixed-Size Chunking (divisão por número fixo de tokens ou caracteres), Sliding Window (sobreposição para manter contexto entre chunks) e Recursive Splitting (divisão baseada na estrutura semântica do texto).\n"
     ]
    }
   ],
   "source": [
    "resposta1 = qa_chain.invoke({\"question\": \"O que é chunking adaptativo?\"})\n",
    "print(resposta1['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d079f8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As principais estratégias de chunking adaptativo são:\n",
      "\n",
      "*   **Fixed-Size Chunking:** Divisão por número fixo de tokens ou caracteres.\n",
      "*   **Sliding Window:** Sobreposição (overlap) para manter o contexto entre os chunks.\n",
      "*   **Recursive Splitting:** Divisão baseada na estrutura semântica do texto.\n"
     ]
    }
   ],
   "source": [
    "resposta2 = qa_chain.invoke({\"question\": \"E quais as principais estratégias?\"})\n",
    "print(resposta2['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "123039ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/16 [00:00<?, ?it/s]Exception raised in Job[9]: IndexError(list index out of range)\n",
      "Evaluating:   6%|▋         | 1/16 [00:02<00:39,  2.67s/it]Exception raised in Job[13]: IndexError(list index out of range)\n",
      "Exception raised in Job[5]: IndexError(list index out of range)\n",
      "Exception raised in Job[1]: IndexError(list index out of range)\n",
      "Evaluating: 100%|██████████| 16/16 [02:13<00:00,  8.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultados da Avaliação com RAGAS:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>context_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O que é RAG e qual problema ele soluciona?</td>\n",
       "      <td>[POR QUE RAG É A REVOLUÇÃO\\nReduz Alucinações\\...</td>\n",
       "      <td>RAG (Retrieval-Augmented Generation) é uma téc...</td>\n",
       "      <td>RAG (Retrieval-Augmented Generation) é uma arq...</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.591667</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quais os componentes essenciais do RAG?</td>\n",
       "      <td>[POR QUE RAG É A REVOLUÇÃO\\nReduz Alucinações\\...</td>\n",
       "      <td>Os componentes essenciais do RAG são:\\n* Embed...</td>\n",
       "      <td>Os componentes essenciais são: Embeddings, Ban...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Qual a diferença entre busca lexical e semântica?</td>\n",
       "      <td>[alura\\nHybrid Search (Busca Híbrida)\\nBusca s...</td>\n",
       "      <td>A busca semântica captura o significado e cont...</td>\n",
       "      <td>Busca lexical (como BM25) encontra correspondê...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O que mede a métrica faithfulness do RAGAS?</td>\n",
       "      <td>[alura\\nMétricas de Geração\\nO RAGAS fornece m...</td>\n",
       "      <td>A métrica faithfulness do RAGAS mede se a resp...</td>\n",
       "      <td>A métrica Faithfulness mede se a resposta gera...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0         O que é RAG e qual problema ele soluciona?   \n",
       "1            Quais os componentes essenciais do RAG?   \n",
       "2  Qual a diferença entre busca lexical e semântica?   \n",
       "3        O que mede a métrica faithfulness do RAGAS?   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [POR QUE RAG É A REVOLUÇÃO\\nReduz Alucinações\\...   \n",
       "1  [POR QUE RAG É A REVOLUÇÃO\\nReduz Alucinações\\...   \n",
       "2  [alura\\nHybrid Search (Busca Híbrida)\\nBusca s...   \n",
       "3  [alura\\nMétricas de Geração\\nO RAGAS fornece m...   \n",
       "\n",
       "                                            response  \\\n",
       "0  RAG (Retrieval-Augmented Generation) é uma téc...   \n",
       "1  Os componentes essenciais do RAG são:\\n* Embed...   \n",
       "2  A busca semântica captura o significado e cont...   \n",
       "3  A métrica faithfulness do RAGAS mede se a resp...   \n",
       "\n",
       "                                           reference  faithfulness  \\\n",
       "0  RAG (Retrieval-Augmented Generation) é uma arq...      0.642857   \n",
       "1  Os componentes essenciais são: Embeddings, Ban...      1.000000   \n",
       "2  Busca lexical (como BM25) encontra correspondê...      1.000000   \n",
       "3  A métrica Faithfulness mede se a resposta gera...      1.000000   \n",
       "\n",
       "   answer_relevancy  context_precision  context_recall  \n",
       "0               NaN           0.591667             1.0  \n",
       "1               NaN           0.500000             1.0  \n",
       "2               NaN           0.916667             1.0  \n",
       "3               NaN           0.750000             1.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "\n",
    "# 1. Crie um conjunto de dados para avaliação\n",
    "perguntas = [\n",
    "    \"O que é RAG e qual problema ele soluciona?\",\n",
    "    \"Quais os componentes essenciais do RAG?\",\n",
    "    \"Qual a diferença entre busca lexical e semântica?\",\n",
    "    \"O que mede a métrica faithfulness do RAGAS?\"\n",
    "]\n",
    "respostas_ouro = [\n",
    "    \"RAG (Retrieval-Augmented Generation) é uma arquitetura que combina um motor de busca para recuperar informações com um LLM para gerar respostas. Ele soluciona problemas como alucinações e conhecimento desatualizado dos LLMs.\",\n",
    "    \"Os componentes essenciais são: Embeddings, Banco de Dados Vetorial, Chunking e um Modelo de Linguagem (LLM).\",\n",
    "    \"Busca lexical (como BM25) encontra correspondências exatas de termos, enquanto a busca semântica captura o significado e o contexto, mesmo com palavras diferentes.\",\n",
    "    \"A métrica Faithfulness mede se a resposta gerada é suportada e factualmente consistente com os documentos recuperados, evitando alucinações.\"\n",
    "]\n",
    "\n",
    "# 2. Gere as respostas e contextos com a nossa cadeia\n",
    "respostas_geradas = []\n",
    "contextos_recuperados = []\n",
    "for question in perguntas:\n",
    "    result = qa_chain.invoke({\"question\": question})\n",
    "    respostas_geradas.append(result['answer'])\n",
    "    contextos_recuperados.append([doc.page_content for doc in result['source_documents']])\n",
    "\n",
    "# 3. Crie o dataset no formato esperado pelo RAGAS\n",
    "dataset_dict = {\n",
    "    'question': perguntas,\n",
    "    'answer': respostas_geradas,\n",
    "    'contexts': contextos_recuperados,\n",
    "    'ground_truth': respostas_ouro\n",
    "}\n",
    "dataset = Dataset.from_dict(dataset_dict)\n",
    "\n",
    "# 4. Execute a avaliação, agora usando os modelos do Google\n",
    "evaluation_result = evaluate(\n",
    "    dataset=dataset,\n",
    "    metrics=[\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        context_precision,\n",
    "        context_recall,\n",
    "    ],\n",
    "    llm=ChatGoogleGenerativeAI(model=\"models/gemini-2.0-flash\"),\n",
    "    embeddings=embeddings_model\n",
    ")\n",
    "\n",
    "# 5. Analise os resultados\n",
    "df_resultados = evaluation_result.to_pandas()\n",
    "print(\"\\nResultados da Avaliação com RAGAS:\")\n",
    "display(df_resultados)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
